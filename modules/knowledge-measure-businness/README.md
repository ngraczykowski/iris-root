# Knowledge-Measure-Business

### Table of contents
 - Intro
 - Installation
 - Walkthrough
 - Custom Features
 - Comments
 - Policies
 - Interfaces

## Intro
The Knowledge-Measure-Business architecture is a self-contained implementation of our alert solving process for
Name Screening (NS, aka Customers) and Transaction Screening (TS, aka Payments) products.

The solution operates on the abstract concept of layers, each with its set of unique, non-overlapping responsibilities:
- **Knowledge**  
This layer is responsible for providing as much information as required on a single data point. The data point can be
of any nature (geo, names, ids, ...) and is analyzed by the single, appropriate service (if available) in this layer or all
of them.
- **Measure**  
The Measure layer leverages the information gathered by the Knowledge layer on single data points and measures the
distance between data point pairs. The distance can be measured in various dimensions: text, time, geolocation, ...
- **Business**  
Finally, the Business layer aggregates all the output from both of the previous layers and transforms it into business
value, outputting the well-known solution consisting of:
    - a recommendation for an alert (Potential True Positive, False Positive, ...)
    - a feature vector representing all of the features extracted from the alert data by the Knowledge and Measure
    layers
    - a human-readable narrative explaining the logic behind the recommendation (aka the comment)

This architecture is designed to be highly customizable and contain client-specific configuration in a single place,
which is not impacting the company as a whole.

This solution is assumed to source the necessary data from the application layer of the ETL platform.

## Installation
Prerequisites:
- create python3.8 env via conda / virtualenv and launch it

Prepare:
```
export PIP_INDEX_URL=https://<USERNAME>:<PASSWORD>@repo.silenteight.com/artifactory/api/pypi/pypi/simple

git submodule update --init --recursive

pip install -r requirements.txt

COMPOSE_DOCKER_CLI_BUILD=1 DOCKER_BUILDKIT=1  docker-compose -f docker/docker-compose.yaml build
```

Use the same USERNAME and PASSWORD of PIP index to authenticate to Silent Eight repo to pull docker images:
```
docker login docker.repo.silenteight.com
```

Launch docker:
```
docker-compose -f docker/docker-compose.yaml up
```

Launch jupyter notebook in the created environment

```
jupyter notebook
```

## Walkthrough
This tool is designed to be controlled primarily through the yaml config files and that's where a lot of the work will
be done. Therefore, let's explore a simple config file.

### Config
```
decisions:
 PTP:
   verbal: Potential True Positive
   comment_type: positive_comment
 FP:
   verbal: False Posisitve
   comment_type: negative_comment
 MI:
   verbal: Manual Investigation
   comment_type: null

comment_intro: "S8 recommended action: {}\n"
generic_positive_comment: &generic_positive_comment "Alerted Party's {context}} ({ap_value}) matches Watchlist Party's {context} ({wl_value})"
generic_negative_comment: &generic_negative_comment "Alerted Party's {context} ({ap_value}) does not match Watchlist Party's {context} ({wl_value})"

feature_mapping:
  services:
    geo:
      measure:
        fields: [nationalities, residencies]
        contexts: [nationality, residency]
        comments:
          negative_comment: *generic_negative_comment
          positive_comment: *generic_positive_comment

  custom_features:
    identity:
      knowledge:
        fields: [message_data]
    sanctions:
      measure:
        fields: [residencies, nationalities]
        contexts: [residency, nationality]
        comments:
          negative_comment: ""
          positive_comment: "Alerted Party's {context} ({ap_value}) is on the sanctions list"
```

The `decisions` block maps out the final decisions our solution will be providing. The label is the key. The values
determine how the decision will appear verbally in the comment (`verbal`) and which comment template should be used
for a given decision (positive for escalations, negative for closures).

The block starting with `comment_intro` is a convenient approach for defining the most commonly used comment templates.

The `feature_mapping` block serves to declare the features that will be generated by the solution. These are split into
multiple levels. Starting from the top:

*domain_source* describes whether a feature is generated by pre-defined `services` or the user defined
`custom_features`.

*domain* is the area in which the feature operates (`geo`, `sanctions`, `org_name`,...)

*domain_type* defines the character of the domain as a `knowledge` and/or `measure` domain

`fields` work differently depending on the domain specifics, in which they are used. For `knowledge` domain types
these define the fields, on which the logic is to be applied. For `measure` domain types these define *at least*
the watchlist fields which will be used for comparison.

`contexts` are exclusive to `measure` domains and describe the verbal value of the context corresponding to each
used field. It is used to produce the final feature name and as verbal input to the comment.

`comments` are also exclusive to `measure` domains and define the comment template associated with an escalation or closure.

> The architecture will produce atomic features - one feature per field.

The naming convention for the generated features follows the schema `{domain_name}_{context}`. Therefore, the example
config above will create the following *knowledge features*: identity_message_data; and the following
*measure features*: geo_residency, geo_nationality, sanctions_residency and sanctions_nationality.

Parsing and validating the config is taken care of by `business_layer/config/config.py`.

### Initialization
The entry point for out solution is the `BusinessLayerNS/TS` classes found in `business_layer/business_layer_ns.py` and
`business_layer/business_layer_ts.py`, respectively. The `solve_hit` method called in these classes represent
the entire flow of our solution. This is a generic class able to handle most predictable customizations added by
the user without the need of modification.

Upon initializing, the class the constructor will gather the *knowledge* and *measure* methods present in the
pre-defined services (these are designed to provide both methods out of the box). It will do the same for the
custom methods defined by the user present in the `business_layer/custom_knowledge` and
`business_layer/custom_measure`, based on the *domain names* and *domain types* in `custom_features` in the
configuration. If a *domain name* is defined, at least one of these types of methods must be defined if the
*domain*.

### Solving
The solution flow for each product is best represented by the `solve_hit` method in the `BusinessLayerXX` class.

#### NS
The solution for NS starts with:
1. Providing measures. <br/>
  Here, the measure methods from the services and custom features will be called in order to provide
  measurements on the fields defined in the config. The NS product usually does not require input from the *knowledge*
  layer as the incoming data is structured and already provides information about its character.

2. Creating the feature vector. <br/>
  Here, the measure recommendations for each feature are aggregated into a vector.

3. Finding matching policies. <br/>
  Here, the feature vector is scanned in order to assess whether it matches any of the defined solution policies.

4. Generating the comment. <br/>
  Here, the appropriate comment is generated based on the matched policy, the decision it enforces and the
  specific values of the data points that have triggered those policies.

5. Compiling output. <br/>
  The final step is to compile and present the output of the solution. Here, the output consists of the
  feature vector, the final decision and the comment.

 #### TS
 The solution for TS is more involved as it requires working with freetext data.
 1. Providing knowledge. <br/>
    The first step is to apply the *knowledge* methods to the freetext TS message. This allows us to gather facts
    from various domains in the message.

 2. Providing measures. <br/>
    As in NS, we trigger the *measure* methods, which now operate on the *knowledge* findings from the previous step.

 3. Applying domain hierarchy. <br/>
    Applying multiple *knowledge* methods on freetext fields is likely to create conflicts between domains.
    For example, applying *organization name knowledge* on "ACME CO LTD" will correctly identify the presence of
    a company with all of its characteristics. One of those is identifying *CO* as a legal extension. However, applying
    *geo knowledge* on the same example will recognize *CO* as: COLOMBIA, COLORADO (and therefore US). To resolve this
    conflict the user may want to define a domain hierarchy. In this case the *organization name* domain may be considered
    "superior" to the *geo* domain and the offending geo measures can be ignored in further analysis.

  4. Creating the feature vector. <br/>
    As in NS.

  5. Finding matching policies. <br/>
    As in NS.

  6. Generating the comment. <br/>
    As in NS

  7. Compiling output. <br/>
    As in NS

## Custom features
The capabilities of pre-defined services may prove insufficient in certain cases. Therefore, you may want to
define your own *knowledge* and/or *measure* methods. The architecture takes this into account and provides the capacity
to freely define both method types.

### Custom knowledge
Defining a custom knowledge feature must follow these steps:
 - start with specifying a *domain* in the `custom_features`
 - within the domain define the *domain type* (`knowledge` in this case)
 - within the domain type define the `fields`, which the domain will provide the knowledge on
 - next, a python file named *exactly* as the domain must be placed under `business_layer/custom_knowledge`
 - the file must contain a class, which name also follows the *domain* name and is capitalized
 - the class must implement the `BaseCustomKnowledge` interface and thus must have a `run` method defined.
 - the `run` method must take in the given hit/alert data as input and return a `List[ValueKnowledge]`

### Custom measure
Defining a custom measure feature must follow these steps:
 - start with specifying a *domain* in the `custom_features`
 - within the domain define the *domain type* (`measure` in this case)
 - within the domain type define the watchlist `fields`, which the domain will use to measure
 - next, a python file named *exactly* as the domain must be placed under `business_layer/custom_measure`
 - the file must contain a class, which name also follows the *domain* name and is capitalized
 - the class must implement the `BaseCustomMeasure` interface and thus must have the `run` and
 the `reduce_value_measure`methods defined and return a `FieldMeasure`
 - the `run` method is designed to take in the given hit/alert data, all knowledge results and all service-provided
 measure results, though not all need to be used.
 - the `reduce_value_measure` method must take the value measures (`List[ValueMeasure]`) as input and define the
 reduction logic for the combination of all measure results for the values present in a given field

Custom measures do not have to have the corresponding custom knowledge defined. However, if a custom measure needs
to use custom knowledge for proper input, it is recommended to define the latter in the proper place
rather than forcing all of the logic into the former.

## Comments

## Policies

## Interfaces

### _class_ FieldMeasure(recommendation, context, results, domain_name=None)


* **Parameters**


    * **recommendation** (*str*) – The field level measure recommendation - reduced from all value results


    * **context** (*str*) – The verbal string describing the field’s meaning


    * **results** (*List[**ValueMeasure**]*) – The value measure results for all values present in the field


    * **domain_name** (*Optional[**str**]= None*) –




<br>


### _class_ ValueKnowledge(original_input, results, domain_name=None)


* **Parameters**


    * **original_input** (*str*) – The exact piece of input data, which created the following results


    * **results** (*Any*) – An object containing the knowledge results acquired from the original_input


    * **domain_name** (*Optional[**str**]= None*) –

<br>

### _class_ ValueMeasure(ap_value, wl_value, evaluation, metrics=None, ignore=False, ignore_reason='')


* **Parameters**


    * **ap_value** (*str*) – The value of a field entry corresponding to one side of the measure
    (canonically the alerted party side)


    * **wl_value** (*str*) – The value of a field entry corresponding to other side of the measure
    (canonically the watchlist party side)


    * **evaluation** (*str*) – Contains the evaluation of the measure between ap_value and wl_value


    * **metrics** (*Any = None*) – Metrics used to produce evaluation


    * **ignore** (*bool = False*) – A flag indicating whether this measure should be ignored in the analysis


    * **ignore_reason** (*str = ""*) – The reason for ignoring the measure
