[APP]
PYTHON_DATA_TOOLBOX_PATH = ../python-datatoolbox/
DATA_DIR = ./data

; oracle or postgres dump or csv files come here
RAW_DATA_DIR = %(DATA_DIR)s/1.raw/

; parquet, delta or other formats, ideally columnar
; data has been extracted, flatten, but not transformaed, e.g, update data type
STANDARDIZED_DATA_DIR = %(DATA_DIR)s/2.standardized/

; apply low level business logic, data transformation, e.g, update data type, create agent input alike
; the input of application/agent should also be from here, e.g, denormalized, df360
CLEANSED_DATA_DIR = %(DATA_DIR)s/3.cleansed/

; filter, extract the needed rows and columns from previous layer
APPLICATION_DATA_DIR = %(DATA_DIR)s/4.application/

; the output from applications
REPORT_DATA_DIR = %(DATA_DIR)s/5.report/

; any adhoc, data science task, data could be from any layer
SANDBOX_DATA_DIR = %(DATA_DIR)s/6.sandbox/

; space for other config files
APPLICATION_CONF_DIR = ../conf/

[SPARK]
APP_NAME = aia-ns-pov
MASTER = local[8]
DRIVER_MEMORY = 8g

; setting this to true to override the above spark master (CPU core) and driver memory
USE_OPTIMAL_CONFIG = true

JAVA_HOME = /app/app/opt/jdk-11
IVY2_DIR = ./dependencies/ivy2
DB_JARS = ./dependencies/ivy2/jars/com.oracle_ojdbc8-12.2.0.1.jar,./dependencies/ivy2/jars/org.postgresql_postgresql-42.2.20.jar
TMP_DIR = /tmp/app/spark
DIR="$(cd -- "$(dirname -- "${0}")" && pwd)"

[SERP]
SERP_DIRECTORY = ./env/serp

[PYTHON]
; skip the setup and install dependancies, usually used in offline environment has no Internet access, but do use with causion
SKIP_PACKAGE_DEPENDENCY = true
; setting below to true to re-compile python-datatoobox to egg. depreciated
# IS_COMPILE_EGG = true

[DATABASE]
USER = 
PASSWORD = 
HOST = 
PORT = 5432
DATABASE = 
;PSQL = psql
PSQL = PGOPTIONS='-c search_path=amlsear' psql
SCHEMA = 

ORACLEHOST = hklpdudasb-scan.hk.standardchartered.com
ORACLEPORT = 1622
ORACLEUSER = LC_GNS_WEB_PT_SO
ORACLEPASSWORD = GNS_LC_123_so
ORACLESERVICE = LC_GNS_SIT.hk.standardchartered.com
