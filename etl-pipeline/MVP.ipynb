{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert raw data to standardized format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== .:python-datatoolbox\n",
      "{'dataset': 'learning_quality', 'input_files_dir': 'inputs', 'artifact_files_dir': 'artifacts', 'crucial_artifact_files_dir': 'crucial_artifacts', 'exportable_files_dir': 'exportable', 'tmp_dir': 'tmp', 'processing_pool_size': 10, 'customer_specific': {'agents': {'name': {'data_availability': {'agent_input': 'agent-name-inputs.csv', 'fields': {'alerted_party_field': 'ad_alerted_name', 'watchlist_field': 'ad_matched_name'}, 'crucial_artifact': 'agent-name-availability.csv'}, 'artifact_file': 'agent-name-results.csv', 'tuning_report_path': 'agent-name-tuning-report.csv'}, 'dob': {'data_availability': {'agent_input': 'agent-dob-inputs.csv', 'fields': {'alerted_party_field': 'ad_alerted_dob', 'watchlist_field': 'ad_matched_dob'}, 'crucial_artifact': 'agent-dob-availability.csv'}, 'artifact_file': 'agent-dob-results.csv'}, 'nationality': {'data_availability': {'agent_input': 'agent-nationality-inputs.csv', 'fields': {'alerted_party_field': 'ad_alerted_nationality', 'watchlist_field': 'ad_matched_nationality'}, 'crucial_artifact': 'agent-nationality-availability.csv'}, 'artifact_file': 'agent-nationality-results.csv'}, 'shoe-size': {'data_availability': {'agent_input': 'agent-shoe-size-inputs.csv', 'fields': {'alerted_party_field': 'ad_alerted_shoe_size', 'watchlist_field': 'ad_matched_shoe_size'}, 'crucial_artifact': 'agent-shoe-size-availability.csv'}, 'artifact_file': 'agent-shoe-size-results.csv'}, 'eye-color': {'data_availability': {'agent_input': 'agent-eye-color-inputs.csv', 'assume_resolvable': True, 'mock_agent': True, 'crucial_artifact': 'eye-color-availability.csv'}, 'artifact_file': 'eye-color-results.csv'}}, 'analyst_resoning_config_file': 'analyst-reasoning-config.ini', 'coverage_score_config_file': 'coverage-score-config.ini', 'external_analyst_sentiments_files': ['external_sample_sentiments.csv'], 'sentiment_blacklists': {'shoe-size': ['missing']}, 'csv_options': {'quote': \"'\", 'escape': '\\\\', 'delimiter': ','}, 'documented_business_questions': ['customer_type', 'is_deny']}, 'input_files': {'filtered_customer_alerts_path': 'filtered-customer-alerts.csv', 'alert_comments_path': 'alert-comments.csv', 'hit_comments_path': 'hit-comments.csv', 'documented_business_questions_path': 'documented-business-questions.csv', 'documented_business_data_path': 'documented-business-data.csv', 'hit_level_analyst_solutions_path': 'hit-level-analyst-solutions.csv', 'hit_level_recommendations_path': 'hit-level-recommendations.csv'}, 'artifact_files': {'commented_business_questions_path': 'commented-business-questions.csv', 'commented_business_sentiments_path': 'commented-business-sentiments.csv', 'all_business_questions_path': 'all-business-questions.csv', 'resolvability_report_path': 'resolvability-report.csv', 'reasoning_branches_path': 'reasoning-branches.csv', 'analyst_results_path': 'analyst-results.csv', 'merged_results_and_solutions_path': 'merged-results-and-solutions.csv'}, 'crucial_artifact_files': {'agents_impact_path': 'agents-impact.csv', 'theoretical_ceiling_path': 'theoretical-ceiling.csv', 'agent_data_availability_path': 'agent-data-availability.csv', 'dad_report_path': 'dad-report.csv', 'current_ceiling_path': 'current-ceiling.csv', 'rb_scorer_report_path': 'rb-scorer-report.csv', 'efficiency_report_path': 'efficiency-report.csv', 'efficiency_history_report_path': 'efficiency-history-report.csv', 'accuracy_report_path': 'accuracy-report.csv', 'false_negative_report_path': 'false-negative-report.csv', 'ai_reasoning_report_path': 'ai-reasoning-report.csv', 'data_availability_analysis': 'data-availability-analysis.csv'}, 'exportable_files': {'dad_report': {'source_dir_property': 'crucial_artifact_files_dir', 'file': 'dad-report.csv', 'without_columns': ['unique_key', 'suspect_id', 'comment']}, 'theoretical_ceiling': {'source_dir_property': 'crucial_artifact_files_dir', 'file': 'theoretical-ceiling.csv'}, 'current_ceiling': {'source_dir_property': 'crucial_artifact_files_dir', 'file': 'current-ceiling.csv'}, 'resolvability_report': {'source_dir_property': 'artifact_files_dir', 'file': 'resolvability-report.csv', 'without_columns': ['unique_key', 'suspect_id', 'comment']}, 'merged_results_and_solutions': {'source_dir_property': 'artifact_files_dir', 'file': 'merged-results-and-solutions.csv', 'without_columns': ['unique_key', 'suspect_id', 'comment']}}, 'spark': {'master': 'local[2]', 'app-name': 'main-pipeline', 'config': {'spark.executor.memory': '512m', 'spark.driver.memory': '4g'}}, 'serp': {'python_client': {'cert': {'user_chain': '../cert/user/user-chain.pem', 'user_key': '../cert/user/user-key.pem', 'ca_certs': '../cert/ca-certs.pem'}, 'consul': {'host': 'localhost', 'port': 24120}}}, 'pipeline': {'name': 'simple', 'result_dir': 'notebooks-outputs/', 'notebooks_path': 'notebooks', 'steps': ['1.0-CS-filter-customer-alerts', '1.5-CS-ETL-hit-comments', '1.9-columns-usability', '2.0-G-CSC-extract-analyst-questions', '2.5-CSC-agents-from-business', '2.6-G-all-agents', '2.7-G-agents-impact', '3.0-data-availability', '3.5.EDA-analysis-of-data-availability', 'additional/3.6-CS-analysis-of-data-availability', '6.1-solve-with-name-agent', '6.2-solve-with-date-agent', '6.3-solve-with-country-agent', '6.4-solve-with-shoe-size-agent', 'additional/6.98-mock-agents-solve', '4.2-merge-agent-results-with-documented-business-data', '7.1-merge-all-business-sentiments', '7.2-merge-analyst-results-with-analyst-solutions', '7.3-RB-scorer', '8.0-agent-tuning-report', '9.0-efficiency-report', '9.1-accuracy-report', '9.2-false-negative-report', '9.3-ai-reasoning-report', 'additional/10.0-crucial-artifacts-summary', 'additional/10.1-exportable-files']}}\n",
      "{'dataset': 'learning_quality', 'input_files_dir': 'inputs', 'artifact_files_dir': 'artifacts', 'crucial_artifact_files_dir': 'crucial_artifacts', 'exportable_files_dir': 'exportable', 'tmp_dir': 'tmp', 'processing_pool_size': 10, 'customer_specific': {'agents': {'name': {'data_availability': {'agent_input': 'agent-name-inputs.csv', 'fields': {'alerted_party_field': 'ad_alerted_name', 'watchlist_field': 'ad_matched_name'}, 'crucial_artifact': 'agent-name-availability.csv'}, 'artifact_file': 'agent-name-results.csv', 'tuning_report_path': 'agent-name-tuning-report.csv'}, 'dob': {'data_availability': {'agent_input': 'agent-dob-inputs.csv', 'fields': {'alerted_party_field': 'ad_alerted_dob', 'watchlist_field': 'ad_matched_dob'}, 'crucial_artifact': 'agent-dob-availability.csv'}, 'artifact_file': 'agent-dob-results.csv'}, 'nationality': {'data_availability': {'agent_input': 'agent-nationality-inputs.csv', 'fields': {'alerted_party_field': 'ad_alerted_nationality', 'watchlist_field': 'ad_matched_nationality'}, 'crucial_artifact': 'agent-nationality-availability.csv'}, 'artifact_file': 'agent-nationality-results.csv'}, 'shoe-size': {'data_availability': {'agent_input': 'agent-shoe-size-inputs.csv', 'fields': {'alerted_party_field': 'ad_alerted_shoe_size', 'watchlist_field': 'ad_matched_shoe_size'}, 'crucial_artifact': 'agent-shoe-size-availability.csv'}, 'artifact_file': 'agent-shoe-size-results.csv'}, 'eye-color': {'data_availability': {'agent_input': 'agent-eye-color-inputs.csv', 'assume_resolvable': True, 'mock_agent': True, 'crucial_artifact': 'eye-color-availability.csv'}, 'artifact_file': 'eye-color-results.csv'}}, 'analyst_resoning_config_file': 'analyst-reasoning-config.ini', 'coverage_score_config_file': 'coverage-score-config.ini', 'external_analyst_sentiments_files': ['external_sample_sentiments.csv'], 'sentiment_blacklists': {'shoe-size': ['missing']}, 'csv_options': {'quote': \"'\", 'escape': '\\\\', 'delimiter': ','}, 'documented_business_questions': ['customer_type', 'is_deny']}, 'input_files': {'filtered_customer_alerts_path': 'filtered-customer-alerts.csv', 'alert_comments_path': 'alert-comments.csv', 'hit_comments_path': 'hit-comments.csv', 'documented_business_questions_path': 'documented-business-questions.csv', 'documented_business_data_path': 'documented-business-data.csv', 'hit_level_analyst_solutions_path': 'hit-level-analyst-solutions.csv', 'hit_level_recommendations_path': 'hit-level-recommendations.csv'}, 'artifact_files': {'commented_business_questions_path': 'commented-business-questions.csv', 'commented_business_sentiments_path': 'commented-business-sentiments.csv', 'all_business_questions_path': 'all-business-questions.csv', 'resolvability_report_path': 'resolvability-report.csv', 'reasoning_branches_path': 'reasoning-branches.csv', 'analyst_results_path': 'analyst-results.csv', 'merged_results_and_solutions_path': 'merged-results-and-solutions.csv'}, 'crucial_artifact_files': {'agents_impact_path': 'agents-impact.csv', 'theoretical_ceiling_path': 'theoretical-ceiling.csv', 'agent_data_availability_path': 'agent-data-availability.csv', 'dad_report_path': 'dad-report.csv', 'current_ceiling_path': 'current-ceiling.csv', 'rb_scorer_report_path': 'rb-scorer-report.csv', 'efficiency_report_path': 'efficiency-report.csv', 'efficiency_history_report_path': 'efficiency-history-report.csv', 'accuracy_report_path': 'accuracy-report.csv', 'false_negative_report_path': 'false-negative-report.csv', 'ai_reasoning_report_path': 'ai-reasoning-report.csv', 'data_availability_analysis': 'data-availability-analysis.csv'}, 'exportable_files': {'dad_report': {'source_dir_property': 'crucial_artifact_files_dir', 'file': 'dad-report.csv', 'without_columns': ['unique_key', 'suspect_id', 'comment']}, 'theoretical_ceiling': {'source_dir_property': 'crucial_artifact_files_dir', 'file': 'theoretical-ceiling.csv'}, 'current_ceiling': {'source_dir_property': 'crucial_artifact_files_dir', 'file': 'current-ceiling.csv'}, 'resolvability_report': {'source_dir_property': 'artifact_files_dir', 'file': 'resolvability-report.csv', 'without_columns': ['unique_key', 'suspect_id', 'comment']}, 'merged_results_and_solutions': {'source_dir_property': 'artifact_files_dir', 'file': 'merged-results-and-solutions.csv', 'without_columns': ['unique_key', 'suspect_id', 'comment']}}, 'spark': {'master': 'local[2]', 'app-name': 'main-pipeline', 'config': {'spark.executor.memory': '512m', 'spark.driver.memory': '4g'}}, 'serp': {'python_client': {'cert': {'user_chain': '../cert/user/user-chain.pem', 'user_key': '../cert/user/user-key.pem', 'ca_certs': '../cert/ca-certs.pem'}, 'consul': {'host': 'localhost', 'port': 24120}}}, 'pipeline': {'name': 'simple', 'result_dir': 'notebooks-outputs/', 'notebooks_path': 'notebooks', 'steps': ['1.0-CS-filter-customer-alerts', '1.5-CS-ETL-hit-comments', '1.9-columns-usability', '2.0-G-CSC-extract-analyst-questions', '2.5-CSC-agents-from-business', '2.6-G-all-agents', '2.7-G-agents-impact', '3.0-data-availability', '3.5.EDA-analysis-of-data-availability', 'additional/3.6-CS-analysis-of-data-availability', '6.1-solve-with-name-agent', '6.2-solve-with-date-agent', '6.3-solve-with-country-agent', '6.4-solve-with-shoe-size-agent', 'additional/6.98-mock-agents-solve', '4.2-merge-agent-results-with-documented-business-data', '7.1-merge-all-business-sentiments', '7.2-merge-analyst-results-with-analyst-solutions', '7.3-RB-scorer', '8.0-agent-tuning-report', '9.0-efficiency-report', '9.1-accuracy-report', '9.2-false-negative-report', '9.3-ai-reasoning-report', 'additional/10.0-crucial-artifacts-summary', 'additional/10.1-exportable-files']}}\n",
      "Available memory: 50845650944\n",
      "Required memory: 4831838208\n",
      "Actual Spark Master: local[10]\n",
      "Actual Spark Driver Memory: 29g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/env/ds/anaconda/envs/pipeline/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /app/dependencies/ivy2/cache\n",
      "The jars for the packages stored in: /app/dependencies/ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-351ab865-3e15-4b31-b66d-53c0c8962b46;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/env/ds/anaconda/envs/pipeline/lib/python3.7/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound io.delta#delta-core_2.12;1.0.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      ":: resolution report :: resolve 141ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.0.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   8   |   0   |   0   |   0   ||   8   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-351ab865-3e15-4b31-b66d-53c0c8962b46\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 8 already retrieved (0kB/4ms)\n",
      "21/12/10 12:02:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "21/12/10 12:02:38 WARN DependencyUtils: Local jar /app/dependencies/ivy2/jars/com.oracle_ojdbc8-12.2.0.1.jar does not exist, skipping.\n",
      "21/12/10 12:02:38 WARN DependencyUtils: Local jar /app/./dependencies/ivy2/jars/org.postgresql_postgresql-42.2.20.jar does not exist, skipping.\n",
      "21/12/10 12:02:38 INFO SparkContext: Running Spark version 3.1.1\n",
      "21/12/10 12:02:38 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "21/12/10 12:02:38 INFO ResourceUtils: ==============================================================\n",
      "21/12/10 12:02:38 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "21/12/10 12:02:38 INFO ResourceUtils: ==============================================================\n",
      "21/12/10 12:02:38 INFO SparkContext: Submitted application: aia-ns-pov\n",
      "21/12/10 12:02:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "21/12/10 12:02:38 INFO ResourceProfile: Limiting resource is cpu\n",
      "21/12/10 12:02:38 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "21/12/10 12:02:38 INFO SecurityManager: Changing view acls to: root\n",
      "21/12/10 12:02:38 INFO SecurityManager: Changing modify acls to: root\n",
      "21/12/10 12:02:38 INFO SecurityManager: Changing view acls groups to: \n",
      "21/12/10 12:02:38 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/12/10 12:02:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "21/12/10 12:02:38 INFO Utils: Successfully started service 'sparkDriver' on port 44685.\n",
      "21/12/10 12:02:39 INFO SparkEnv: Registering MapOutputTracker\n",
      "21/12/10 12:02:39 INFO SparkEnv: Registering BlockManagerMaster\n",
      "21/12/10 12:02:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "21/12/10 12:02:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "21/12/10 12:02:39 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "21/12/10 12:02:39 INFO DiskBlockManager: Created local directory at /tmp/app/spark/blockmgr-9d80fe08-10be-4d2e-aa2d-0abc71519a96\n",
      "21/12/10 12:02:39 INFO MemoryStore: MemoryStore started with capacity 17.2 GiB\n",
      "21/12/10 12:02:39 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "21/12/10 12:02:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "21/12/10 12:02:39 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://309651fbdaab:4040\n",
      "21/12/10 12:02:39 ERROR SparkContext: Failed to add /app/dependencies/ivy2/jars/com.oracle_ojdbc8-12.2.0.1.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /app/dependencies/ivy2/jars/com.oracle_ojdbc8-12.2.0.1.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1935)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:1987)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:501)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:501)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:501)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "21/12/10 12:02:39 ERROR SparkContext: Failed to add ./dependencies/ivy2/jars/org.postgresql_postgresql-42.2.20.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /app/dependencies/ivy2/jars/org.postgresql_postgresql-42.2.20.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1935)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:1987)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:501)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:501)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:501)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "21/12/10 12:02:39 INFO SparkContext: Added file file:///app/dependencies/ivy2/jars/io.delta_delta-core_2.12-1.0.0.jar at file:///app/dependencies/ivy2/jars/io.delta_delta-core_2.12-1.0.0.jar with timestamp 1639137758791\n",
      "21/12/10 12:02:39 INFO Utils: Copying /app/dependencies/ivy2/jars/io.delta_delta-core_2.12-1.0.0.jar to /tmp/app/spark/spark-b4f75a4c-de12-4e0d-89fd-1bb64fc96015/userFiles-b97defce-65a3-4597-bd8d-92ff232a786e/io.delta_delta-core_2.12-1.0.0.jar\n",
      "21/12/10 12:02:39 INFO SparkContext: Added file file:///app/dependencies/ivy2/jars/org.antlr_antlr4-4.7.jar at file:///app/dependencies/ivy2/jars/org.antlr_antlr4-4.7.jar with timestamp 1639137758791\n",
      "21/12/10 12:02:39 INFO Utils: Copying /app/dependencies/ivy2/jars/org.antlr_antlr4-4.7.jar to /tmp/app/spark/spark-b4f75a4c-de12-4e0d-89fd-1bb64fc96015/userFiles-b97defce-65a3-4597-bd8d-92ff232a786e/org.antlr_antlr4-4.7.jar\n",
      "21/12/10 12:02:39 INFO SparkContext: Added file file:///app/dependencies/ivy2/jars/org.antlr_antlr4-runtime-4.7.jar at file:///app/dependencies/ivy2/jars/org.antlr_antlr4-runtime-4.7.jar with timestamp 1639137758791\n",
      "21/12/10 12:02:39 INFO Utils: Copying /app/dependencies/ivy2/jars/org.antlr_antlr4-runtime-4.7.jar to /tmp/app/spark/spark-b4f75a4c-de12-4e0d-89fd-1bb64fc96015/userFiles-b97defce-65a3-4597-bd8d-92ff232a786e/org.antlr_antlr4-runtime-4.7.jar\n",
      "21/12/10 12:02:39 INFO SparkContext: Added file file:///app/dependencies/ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar at file:///app/dependencies/ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar with timestamp 1639137758791\n",
      "21/12/10 12:02:39 INFO Utils: Copying /app/dependencies/ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar to /tmp/app/spark/spark-b4f75a4c-de12-4e0d-89fd-1bb64fc96015/userFiles-b97defce-65a3-4597-bd8d-92ff232a786e/org.antlr_antlr-runtime-3.5.2.jar\n",
      "21/12/10 12:02:39 INFO SparkContext: Added file file:///app/dependencies/ivy2/jars/org.antlr_ST4-4.0.8.jar at file:///app/dependencies/ivy2/jars/org.antlr_ST4-4.0.8.jar with timestamp 1639137758791\n",
      "21/12/10 12:02:39 INFO Utils: Copying /app/dependencies/ivy2/jars/org.antlr_ST4-4.0.8.jar to /tmp/app/spark/spark-b4f75a4c-de12-4e0d-89fd-1bb64fc96015/userFiles-b97defce-65a3-4597-bd8d-92ff232a786e/org.antlr_ST4-4.0.8.jar\n",
      "21/12/10 12:02:39 INFO SparkContext: Added file file:///app/dependencies/ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar at file:///app/dependencies/ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar with timestamp 1639137758791\n",
      "21/12/10 12:02:39 INFO Utils: Copying /app/dependencies/ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar to /tmp/app/spark/spark-b4f75a4c-de12-4e0d-89fd-1bb64fc96015/userFiles-b97defce-65a3-4597-bd8d-92ff232a786e/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar\n",
      "21/12/10 12:02:39 INFO SparkContext: Added file file:///app/dependencies/ivy2/jars/org.glassfish_javax.json-1.0.4.jar at file:///app/dependencies/ivy2/jars/org.glassfish_javax.json-1.0.4.jar with timestamp 1639137758791\n",
      "21/12/10 12:02:39 INFO Utils: Copying /app/dependencies/ivy2/jars/org.glassfish_javax.json-1.0.4.jar to /tmp/app/spark/spark-b4f75a4c-de12-4e0d-89fd-1bb64fc96015/userFiles-b97defce-65a3-4597-bd8d-92ff232a786e/org.glassfish_javax.json-1.0.4.jar\n",
      "21/12/10 12:02:39 INFO SparkContext: Added file file:///app/dependencies/ivy2/jars/com.ibm.icu_icu4j-58.2.jar at file:///app/dependencies/ivy2/jars/com.ibm.icu_icu4j-58.2.jar with timestamp 1639137758791\n",
      "21/12/10 12:02:39 INFO Utils: Copying /app/dependencies/ivy2/jars/com.ibm.icu_icu4j-58.2.jar to /tmp/app/spark/spark-b4f75a4c-de12-4e0d-89fd-1bb64fc96015/userFiles-b97defce-65a3-4597-bd8d-92ff232a786e/com.ibm.icu_icu4j-58.2.jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/10 12:02:39 INFO Executor: Starting executor ID driver on host 309651fbdaab\n",
      "21/12/10 12:02:39 INFO Executor: Fetching file:///app/dependencies/ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar with timestamp 1639137758791\n",
      "21/12/10 12:02:39 INFO Utils: /app/dependencies/ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar has been previously copied to /tmp/app/spark/spark-b4f75a4c-de12-4e0d-89fd-1bb64fc96015/userFiles-b97defce-65a3-4597-bd8d-92ff232a786e/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar\n",
      "21/12/10 12:02:39 INFO Executor: Fetching file:///app/dependencies/ivy2/jars/org.antlr_antlr4-4.7.jar with timestamp 1639137758791\n",
      "21/12/10 12:02:39 INFO Utils: /app/dependencies/ivy2/jars/org.antlr_antlr4-4.7.jar has been previously copied to /tmp/app/spark/spark-b4f75a4c-de12-4e0d-89fd-1bb64fc96015/userFiles-b97defce-65a3-4597-bd8d-92ff232a786e/org.antlr_antlr4-4.7.jar\n",
      "21/12/10 12:02:39 INFO Executor: Fetching file:///app/dependencies/ivy2/jars/org.glassfish_javax.json-1.0.4.jar with timestamp 1639137758791\n",
      "21/12/10 12:02:39 INFO Utils: /app/dependencies/ivy2/jars/org.glassfish_javax.json-1.0.4.jar has been previously copied to /tmp/app/spark/spark-b4f75a4c-de12-4e0d-89fd-1bb64fc96015/userFiles-b97defce-65a3-4597-bd8d-92ff232a786e/org.glassfish_javax.json-1.0.4.jar\n",
      "21/12/10 12:02:39 INFO Executor: Fetching file:///app/dependencies/ivy2/jars/io.delta_delta-core_2.12-1.0.0.jar with timestamp 1639137758791\n",
      "21/12/10 12:02:39 INFO Utils: /app/dependencies/ivy2/jars/io.delta_delta-core_2.12-1.0.0.jar has been previously copied to /tmp/app/spark/spark-b4f75a4c-de12-4e0d-89fd-1bb64fc96015/userFiles-b97defce-65a3-4597-bd8d-92ff232a786e/io.delta_delta-core_2.12-1.0.0.jar\n",
      "21/12/10 12:02:39 INFO Executor: Fetching file:///app/dependencies/ivy2/jars/org.antlr_antlr4-runtime-4.7.jar with timestamp 1639137758791\n",
      "21/12/10 12:02:39 INFO Utils: /app/dependencies/ivy2/jars/org.antlr_antlr4-runtime-4.7.jar has been previously copied to /tmp/app/spark/spark-b4f75a4c-de12-4e0d-89fd-1bb64fc96015/userFiles-b97defce-65a3-4597-bd8d-92ff232a786e/org.antlr_antlr4-runtime-4.7.jar\n",
      "21/12/10 12:02:39 INFO Executor: Fetching file:///app/dependencies/ivy2/jars/com.ibm.icu_icu4j-58.2.jar with timestamp 1639137758791\n",
      "21/12/10 12:02:39 INFO Utils: /app/dependencies/ivy2/jars/com.ibm.icu_icu4j-58.2.jar has been previously copied to /tmp/app/spark/spark-b4f75a4c-de12-4e0d-89fd-1bb64fc96015/userFiles-b97defce-65a3-4597-bd8d-92ff232a786e/com.ibm.icu_icu4j-58.2.jar\n",
      "21/12/10 12:02:39 INFO Executor: Fetching file:///app/dependencies/ivy2/jars/org.antlr_ST4-4.0.8.jar with timestamp 1639137758791\n",
      "21/12/10 12:02:39 INFO Utils: /app/dependencies/ivy2/jars/org.antlr_ST4-4.0.8.jar has been previously copied to /tmp/app/spark/spark-b4f75a4c-de12-4e0d-89fd-1bb64fc96015/userFiles-b97defce-65a3-4597-bd8d-92ff232a786e/org.antlr_ST4-4.0.8.jar\n",
      "21/12/10 12:02:39 INFO Executor: Fetching file:///app/dependencies/ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar with timestamp 1639137758791\n",
      "21/12/10 12:02:39 INFO Utils: /app/dependencies/ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar has been previously copied to /tmp/app/spark/spark-b4f75a4c-de12-4e0d-89fd-1bb64fc96015/userFiles-b97defce-65a3-4597-bd8d-92ff232a786e/org.antlr_antlr-runtime-3.5.2.jar\n",
      "21/12/10 12:02:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43815.\n",
      "21/12/10 12:02:39 INFO NettyBlockTransferService: Server created on 309651fbdaab:43815\n",
      "21/12/10 12:02:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "21/12/10 12:02:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 309651fbdaab, 43815, None)\n",
      "21/12/10 12:02:39 INFO BlockManagerMasterEndpoint: Registering block manager 309651fbdaab:43815 with 17.2 GiB RAM, BlockManagerId(driver, 309651fbdaab, 43815, None)\n",
      "21/12/10 12:02:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 309651fbdaab, 43815, None)\n",
      "21/12/10 12:02:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 309651fbdaab, 43815, None)\n",
      "21/12/10 12:02:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/app/spark-warehouse').\n",
      "21/12/10 12:02:39 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.\n",
      "21/12/10 12:02:39 INFO SparkContext: Added file /app/dependencies/ivy2/cache/io.delta/delta-core_2.12/jars/delta-core_2.12-1.0.0.jar at file:/app/dependencies/ivy2/cache/io.delta/delta-core_2.12/jars/delta-core_2.12-1.0.0.jar with timestamp 1639137759956\n",
      "21/12/10 12:02:39 INFO Utils: Copying /app/dependencies/ivy2/cache/io.delta/delta-core_2.12/jars/delta-core_2.12-1.0.0.jar to /tmp/app/spark/spark-b4f75a4c-de12-4e0d-89fd-1bb64fc96015/userFiles-b97defce-65a3-4597-bd8d-92ff232a786e/delta-core_2.12-1.0.0.jar\n",
      "2021/12/10 12:02:39 - root INFO: Test info log, from logging\n",
      "21/12/10 12:02:39 WARN aia-ns-pov: Test warn log\n",
      "2021/12/10 12:02:39 - root WARNING: Test warn log, from logging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pySpark version: 3.1.1\n",
      "Spark UI - http://309651fbdaab:4040\n"
     ]
    }
   ],
   "source": [
    "from pipeline.preprocessors import convert_to_standardized, spark_instance, RAW_DATA_DIR, STANDARDIZED_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021/12/10 12:02:58 - root INFO: Start to process ./data/1.raw/ACM_ALERT_NOTES.csv\n",
      "2021/12/10 12:03:04 - root INFO: Data saved to ./data/2.standardized/ACM_ALERT_NOTES.delta\n",
      "2021/12/10 12:03:04 - root INFO: Time lapsed 6.01 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021/12/10 12:03:05 - root INFO: Start to process ./data/1.raw/ACM_MD_ALERT_STATUSES.csv\n",
      "2021/12/10 12:03:07 - root INFO: Data saved to ./data/2.standardized/ACM_MD_ALERT_STATUSES.delta\n",
      "2021/12/10 12:03:07 - root INFO: Time lapsed 1.41 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021/12/10 12:03:08 - root INFO: Start to process ./data/1.raw/ACM_ITEM_STATUS_HISTORY.csv\n",
      "2021/12/10 12:03:09 - root INFO: Data saved to ./data/2.standardized/ACM_ITEM_STATUS_HISTORY.delta\n",
      "2021/12/10 12:03:09 - root INFO: Time lapsed 1.22 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021/12/10 12:03:10 - root INFO: Start to process ./data/1.raw/ALERTS.csv\n",
      "21/12/10 12:03:10 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "2021/12/10 12:03:11 - root INFO: Data saved to ./data/2.standardized/ALERTS.delta\n",
      "2021/12/10 12:03:11 - root INFO: Time lapsed 1.15 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "convert_to_standardized(RAW_DATA_DIR, STANDARDIZED_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create cleansed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.preprocessors import convert_standardized_to_cleansed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension 3 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/10 12:03:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "21/12/10 12:03:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "21/12/10 12:03:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "ANTLR Tool version 4.7 used for code generation does not match the current runtime version 4.8ANTLR Tool version 4.7 used for code generation does not match the current runtime version 4.8"
     ]
    }
   ],
   "source": [
    "convert_standardized_to_cleansed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare agent inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021/12/10 12:03:50 - root INFO: Agent: rba_agent, Input written to ./data/4.application/agent-input/party_type_agent_input.delta, elapsed time: 1.08s\n",
      "2021/12/10 12:03:51 - root INFO: Agent: rba_agent, Input written to ./data/4.application/agent-input/name_agent_input.delta, elapsed time: 1.05s\n",
      "2021/12/10 12:03:52 - root INFO: Agent: rba_agent, Input written to ./data/4.application/agent-input/dob_agent_input.delta, elapsed time: 1.03s\n",
      "2021/12/10 12:03:53 - root INFO: Agent: rba_agent, Input written to ./data/4.application/agent-input/pob_agent_input.delta, elapsed time: 1.05s\n",
      "2021/12/10 12:03:54 - root INFO: Agent: rba_agent, Input written to ./data/4.application/agent-input/gender_agent_input.delta, elapsed time: 1.02s\n",
      "2021/12/10 12:03:56 - root INFO: Agent: rba_agent, Input written to ./data/4.application/agent-input/national_id_agent_input.delta, elapsed time: 1.06s\n",
      "2021/12/10 12:03:57 - root INFO: Agent: rba_agent, Input written to ./data/4.application/agent-input/document_number_agent_input.delta, elapsed time: 1.05s\n",
      "2021/12/10 12:03:58 - root INFO: Agent: rba_agent, Input written to ./data/4.application/agent-input/nationality_agent_input.delta, elapsed time: 1.02s\n",
      "2021/12/10 12:03:59 - root INFO: Agent: rba_agent, Input written to ./data/4.application/agent-input/historical_decision_name_agent_input.delta, elapsed time: 1.05s\n",
      "2021/12/10 12:04:00 - root INFO: Agent: rba_agent, Input written to ./data/4.application/agent-input/pep_payment_agent_input.delta, elapsed time: 1.06s\n",
      "2021/12/10 12:04:01 - root INFO: Agent: rba_agent, Input written to ./data/4.application/agent-input/hit_is_san_agent_input.delta, elapsed time: 1.04s\n",
      "2021/12/10 12:04:02 - root INFO: Agent: rba_agent, Input written to ./data/4.application/agent-input/hit_is_deceased_agent_input.delta, elapsed time: 1.06s\n",
      "2021/12/10 12:04:03 - root INFO: Agent: rba_agent, Input written to ./data/4.application/agent-input/hit_has_dob_id_address_agent_input.delta, elapsed time: 1.01s\n",
      "2021/12/10 12:04:04 - root INFO: Agent: rba_agent, Input written to ./data/4.application/agent-input/rba_agent_input.delta, elapsed time: 1.02s\n"
     ]
    }
   ],
   "source": [
    "from pipeline.preprocessors import transform_cleansed_to_application\n",
    "transform_cleansed_to_application()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Detailed implementation__\n",
    "\n",
    "It's rather easy to implement if the goal is just to produce the dataframe for agent to consume. Some interim data need to be created to serve the purpose of analytics.\n",
    "\n",
    "There are 2 main categories of transformations.\n",
    "1. Interface/config transformation: Activities on the agent input config/interface.\n",
    "1. Data transformation: Activities on the data based on the config/interface.\n",
    "\n",
    "Steps\n",
    "1. Create the agent input config.\n",
    "    1. Interface/config transformation. Define agent input template. Each agent's input is a dictionary with 4 key-value pairs.\n",
    "    ```\n",
    "    {\n",
    "        'ap': [],\n",
    "        'ap_aliases': [],\n",
    "        'wl': [] ,\n",
    "        'wl_aliases': []\n",
    "    }\n",
    "    ```\n",
    "\n",
    "        - `ap`: The primary value(s) of alerted party's specific attribute, e.g, name, it could be from one or multiple columns.\n",
    "        - `ap_aliases`: The aliases of alerted party's specific attribute, it could be from one or multiple columns.\n",
    "        - So on and so forth for `wl` and `wl_aliases`.\n",
    "\n",
    "    1. Interface/config transformation. Define the list of agents. __Each agent's name must end with `_agent`.__\n",
    "    ```\n",
    "    agent_list = [\n",
    "        'name_agent',\n",
    "        'gender_agent'\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    1. Interface/config transformation. Config the agent input by specifying which column(s) should be treated as the input of which agent's which party's primary or aliase value(s). \n",
    "    ```\n",
    "    {\n",
    "        'name_agent': {\n",
    "            'ap': ['record_name', 'short_name'],\n",
    "            'ap_aliases': ['alternate_name'],\n",
    "            'wl': ['name_hit'],\n",
    "            'wl_aliases': []\n",
    "        },\n",
    "        'gender_agent': {'ap': ['record_gender'],\n",
    "                         'ap_aliases': [],\n",
    "                         'wl': ['additional_infos_gender'],\n",
    "                         'wl_aliases': []\n",
    "                        },\n",
    "    }\n",
    "    ```\n",
    "    Certain concepts need to be defined here.\n",
    "        1. `level-1-key`: The name of each agent, it's `name_agent` and `gender_agent`.\n",
    "        1. `level-1-value`: The value of each agent's config, it's a dictionary, e.g, \n",
    "        ```\n",
    "        {\n",
    "            'ap': ['record_name', 'short_name'],\n",
    "            'ap_aliases': ['alternate_name'],\n",
    "            'wl': ['name_hit'],\n",
    "            'wl_aliases': []\n",
    "        }\n",
    "        ```\n",
    "        1. `level-2-key`: The key of each agent config's value, or rather the key of `level-1-value`. It's `ap`, `ap_aliases`, `wl` and `wl_aliases`.\n",
    "        1. `level-2-value`: The list of column names, e.g, `['record_name', 'short_name']`.\n",
    "\n",
    "1. Create the interim agent input config and data. The interface is standardized from here onwards.  In reality, the data format can be more complex, e.g, national IDs we need to consider both type and document number.\n",
    "    1. Interface/config transformation. Prepend `level-1-key` to `level-2-key` so that `level-2-key` can be used as new column names to host the interim data for analytics and/or debugging activites. Take `name_agent` for example.\n",
    "    ```\n",
    "    {\n",
    "        'name_agent': {\n",
    "            'name_agent_ap': ['record_name', 'short_name'],\n",
    "            'name_agent_ap_aliases': ['alternate_name'],\n",
    "            'name_agent_wl': ['name_hit'],\n",
    "            'name_agent_wl_aliases': []\n",
    "        }\n",
    "    }\n",
    "    ```\n",
    "    1. Data transformation. Merge the values from `level-2-value` columns to `level-2-key` column. Below table will be the result.\n",
    "    \n",
    "| uuid | record_name | short_name | alternate_name | wl_primary_name | name_hit |   name_agent_ap   | name_agent_ap_aliases | name_agent_wl | name_agent_wl_aliases |\n",
    "| ---- | :---------: | :--------: | :------------: | :-------------: | :------: | :---------------: | :-------------------: | :-----------: | :-------------------: |\n",
    "| 1234 |  Jim Green  |    J.G.    |      Jim       |   James Greg    |   J.G    | [Jim Green, J.G.] |          Jim          |      J.G      |         None          |\n",
    "\n",
    "1. Create the final agent input config and data based on the standardized interface.\n",
    "    1. Interface/config transformation. Now we have a consistent schema to create the 1 list of alerted party values and 1 list of watchlist party values. We no longer need to worry about the customer specific schema, e.g, `record_name`, `short_name` and etcs. They have been standardized as `name_agent_ap`, `name_agent_ap_aliases` and etcs.\n",
    "    ```\n",
    "    {\n",
    "        'name_agent': {'ap_all_names_aggregated': ['name_agent_ap', 'name_agent_ap_aliases'],\n",
    "                       'wl_all_names_aggregated': ['name_agent_wl', 'name_agent_wl_aliases']\n",
    "                      }\n",
    "    }\n",
    "    ```\n",
    "    1. Data transformation. Merge the values from the primary and alias columns. Below table will be the result.\n",
    "    \n",
    "| uuid | ap_all_names_aggregated | wl_all_names_aggregated |\n",
    "| ---- | :---------------------: | :---------------------: |\n",
    "| 1234 | [Jim Green, J.G., Jim]  |          [J.G]          |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
