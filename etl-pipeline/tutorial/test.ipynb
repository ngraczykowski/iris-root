{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "065da429-be46-4c52-961c-4e9ac862a1b6",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "044c1969-9de9-45b1-8e9e-ebed9dd0d20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/ds/anaconda/envs/pipeline/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/env/ds/anaconda/envs/pipeline/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /app/tutorial/dependencies/ivy2/cache\n",
      "The jars for the packages stored in: /app/tutorial/dependencies/ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1091f4a0-045a-4e3f-bdbf-aa64156f3436;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/env/ds/anaconda/envs/pipeline/lib/python3.7/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound io.delta#delta-core_2.12;1.0.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      ":: resolution report :: resolve 160ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.0.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   8   |   0   |   0   |   0   ||   8   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1091f4a0-045a-4e3f-bdbf-aa64156f3436\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 8 already retrieved (0kB/5ms)\n",
      "22/03/02 21:17:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/03/02 21:17:01 WARN DependencyUtils: Local jar /app/tutorial/./dependencies/ivy2/jars/com.oracle_ojdbc8-12.2.0.1.jar does not exist, skipping.\n",
      "22/03/02 21:17:01 WARN DependencyUtils: Local jar /app/tutorial/./dependencies/ivy2/jars/org.postgresql_postgresql-42.2.20.jar does not exist, skipping.\n",
      "22/03/02 21:17:01 INFO SparkContext: Running Spark version 3.1.1\n",
      "22/03/02 21:17:01 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "22/03/02 21:17:01 INFO ResourceUtils: ==============================================================\n",
      "22/03/02 21:17:01 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "22/03/02 21:17:01 INFO ResourceUtils: ==============================================================\n",
      "22/03/02 21:17:01 INFO SparkContext: Submitted application: etl-pipeline-pov\n",
      "22/03/02 21:17:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 8192, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "22/03/02 21:17:01 INFO ResourceProfile: Limiting resource is cpu\n",
      "22/03/02 21:17:01 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "22/03/02 21:17:01 INFO SecurityManager: Changing view acls to: root\n",
      "22/03/02 21:17:01 INFO SecurityManager: Changing modify acls to: root\n",
      "22/03/02 21:17:01 INFO SecurityManager: Changing view acls groups to: \n",
      "22/03/02 21:17:01 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/03/02 21:17:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "22/03/02 21:17:01 INFO Utils: Successfully started service 'sparkDriver' on port 41939.\n",
      "22/03/02 21:17:01 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/03/02 21:17:01 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/03/02 21:17:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/03/02 21:17:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/03/02 21:17:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/03/02 21:17:01 INFO DiskBlockManager: Created local directory at /tmp/app/spark/blockmgr-bbd9b6a2-7338-4a2c-8557-b3801d428bc9\n",
      "22/03/02 21:17:01 INFO MemoryStore: MemoryStore started with capacity 4.6 GiB\n",
      "22/03/02 21:17:01 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/03/02 21:17:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "22/03/02 21:17:01 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://fb8e47287a7e:4040\n",
      "22/03/02 21:17:01 ERROR SparkContext: Failed to add ./dependencies/ivy2/jars/com.oracle_ojdbc8-12.2.0.1.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /app/tutorial/dependencies/ivy2/jars/com.oracle_ojdbc8-12.2.0.1.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1935)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:1987)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:501)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:501)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:501)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/03/02 21:17:01 ERROR SparkContext: Failed to add ./dependencies/ivy2/jars/org.postgresql_postgresql-42.2.20.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /app/tutorial/dependencies/ivy2/jars/org.postgresql_postgresql-42.2.20.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1935)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:1987)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:501)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:501)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:501)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/03/02 21:17:01 INFO SparkContext: Added file file:///app/tutorial/dependencies/ivy2/jars/io.delta_delta-core_2.12-1.0.0.jar at file:///app/tutorial/dependencies/ivy2/jars/io.delta_delta-core_2.12-1.0.0.jar with timestamp 1646255821316\n",
      "22/03/02 21:17:01 INFO Utils: Copying /app/tutorial/dependencies/ivy2/jars/io.delta_delta-core_2.12-1.0.0.jar to /tmp/app/spark/spark-0a61a0b7-4b05-4402-9a81-4a9bc83aede7/userFiles-848bb9be-2112-45f1-b757-00d4f1566811/io.delta_delta-core_2.12-1.0.0.jar\n",
      "22/03/02 21:17:01 INFO SparkContext: Added file file:///app/tutorial/dependencies/ivy2/jars/org.antlr_antlr4-4.7.jar at file:///app/tutorial/dependencies/ivy2/jars/org.antlr_antlr4-4.7.jar with timestamp 1646255821316\n",
      "22/03/02 21:17:01 INFO Utils: Copying /app/tutorial/dependencies/ivy2/jars/org.antlr_antlr4-4.7.jar to /tmp/app/spark/spark-0a61a0b7-4b05-4402-9a81-4a9bc83aede7/userFiles-848bb9be-2112-45f1-b757-00d4f1566811/org.antlr_antlr4-4.7.jar\n",
      "22/03/02 21:17:01 INFO SparkContext: Added file file:///app/tutorial/dependencies/ivy2/jars/org.antlr_antlr4-runtime-4.7.jar at file:///app/tutorial/dependencies/ivy2/jars/org.antlr_antlr4-runtime-4.7.jar with timestamp 1646255821316\n",
      "22/03/02 21:17:01 INFO Utils: Copying /app/tutorial/dependencies/ivy2/jars/org.antlr_antlr4-runtime-4.7.jar to /tmp/app/spark/spark-0a61a0b7-4b05-4402-9a81-4a9bc83aede7/userFiles-848bb9be-2112-45f1-b757-00d4f1566811/org.antlr_antlr4-runtime-4.7.jar\n",
      "22/03/02 21:17:01 INFO SparkContext: Added file file:///app/tutorial/dependencies/ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar at file:///app/tutorial/dependencies/ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar with timestamp 1646255821316\n",
      "22/03/02 21:17:01 INFO Utils: Copying /app/tutorial/dependencies/ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar to /tmp/app/spark/spark-0a61a0b7-4b05-4402-9a81-4a9bc83aede7/userFiles-848bb9be-2112-45f1-b757-00d4f1566811/org.antlr_antlr-runtime-3.5.2.jar\n",
      "22/03/02 21:17:01 INFO SparkContext: Added file file:///app/tutorial/dependencies/ivy2/jars/org.antlr_ST4-4.0.8.jar at file:///app/tutorial/dependencies/ivy2/jars/org.antlr_ST4-4.0.8.jar with timestamp 1646255821316\n",
      "22/03/02 21:17:01 INFO Utils: Copying /app/tutorial/dependencies/ivy2/jars/org.antlr_ST4-4.0.8.jar to /tmp/app/spark/spark-0a61a0b7-4b05-4402-9a81-4a9bc83aede7/userFiles-848bb9be-2112-45f1-b757-00d4f1566811/org.antlr_ST4-4.0.8.jar\n",
      "22/03/02 21:17:01 INFO SparkContext: Added file file:///app/tutorial/dependencies/ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar at file:///app/tutorial/dependencies/ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar with timestamp 1646255821316\n",
      "22/03/02 21:17:01 INFO Utils: Copying /app/tutorial/dependencies/ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar to /tmp/app/spark/spark-0a61a0b7-4b05-4402-9a81-4a9bc83aede7/userFiles-848bb9be-2112-45f1-b757-00d4f1566811/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar\n",
      "22/03/02 21:17:01 INFO SparkContext: Added file file:///app/tutorial/dependencies/ivy2/jars/org.glassfish_javax.json-1.0.4.jar at file:///app/tutorial/dependencies/ivy2/jars/org.glassfish_javax.json-1.0.4.jar with timestamp 1646255821316\n",
      "22/03/02 21:17:01 INFO Utils: Copying /app/tutorial/dependencies/ivy2/jars/org.glassfish_javax.json-1.0.4.jar to /tmp/app/spark/spark-0a61a0b7-4b05-4402-9a81-4a9bc83aede7/userFiles-848bb9be-2112-45f1-b757-00d4f1566811/org.glassfish_javax.json-1.0.4.jar\n",
      "22/03/02 21:17:01 INFO SparkContext: Added file file:///app/tutorial/dependencies/ivy2/jars/com.ibm.icu_icu4j-58.2.jar at file:///app/tutorial/dependencies/ivy2/jars/com.ibm.icu_icu4j-58.2.jar with timestamp 1646255821316\n",
      "22/03/02 21:17:01 INFO Utils: Copying /app/tutorial/dependencies/ivy2/jars/com.ibm.icu_icu4j-58.2.jar to /tmp/app/spark/spark-0a61a0b7-4b05-4402-9a81-4a9bc83aede7/userFiles-848bb9be-2112-45f1-b757-00d4f1566811/com.ibm.icu_icu4j-58.2.jar\n",
      "22/03/02 21:17:01 INFO Executor: Starting executor ID driver on host fb8e47287a7e\n",
      "22/03/02 21:17:01 INFO Executor: Fetching file:///app/tutorial/dependencies/ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar with timestamp 1646255821316\n",
      "22/03/02 21:17:01 INFO Utils: /app/tutorial/dependencies/ivy2/jars/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar has been previously copied to /tmp/app/spark/spark-0a61a0b7-4b05-4402-9a81-4a9bc83aede7/userFiles-848bb9be-2112-45f1-b757-00d4f1566811/org.abego.treelayout_org.abego.treelayout.core-1.0.3.jar\n",
      "22/03/02 21:17:01 INFO Executor: Fetching file:///app/tutorial/dependencies/ivy2/jars/com.ibm.icu_icu4j-58.2.jar with timestamp 1646255821316\n",
      "22/03/02 21:17:01 INFO Utils: /app/tutorial/dependencies/ivy2/jars/com.ibm.icu_icu4j-58.2.jar has been previously copied to /tmp/app/spark/spark-0a61a0b7-4b05-4402-9a81-4a9bc83aede7/userFiles-848bb9be-2112-45f1-b757-00d4f1566811/com.ibm.icu_icu4j-58.2.jar\n",
      "22/03/02 21:17:01 INFO Executor: Fetching file:///app/tutorial/dependencies/ivy2/jars/org.antlr_antlr4-runtime-4.7.jar with timestamp 1646255821316\n",
      "22/03/02 21:17:01 INFO Utils: /app/tutorial/dependencies/ivy2/jars/org.antlr_antlr4-runtime-4.7.jar has been previously copied to /tmp/app/spark/spark-0a61a0b7-4b05-4402-9a81-4a9bc83aede7/userFiles-848bb9be-2112-45f1-b757-00d4f1566811/org.antlr_antlr4-runtime-4.7.jar\n",
      "22/03/02 21:17:01 INFO Executor: Fetching file:///app/tutorial/dependencies/ivy2/jars/org.glassfish_javax.json-1.0.4.jar with timestamp 1646255821316\n",
      "22/03/02 21:17:01 INFO Utils: /app/tutorial/dependencies/ivy2/jars/org.glassfish_javax.json-1.0.4.jar has been previously copied to /tmp/app/spark/spark-0a61a0b7-4b05-4402-9a81-4a9bc83aede7/userFiles-848bb9be-2112-45f1-b757-00d4f1566811/org.glassfish_javax.json-1.0.4.jar\n",
      "22/03/02 21:17:01 INFO Executor: Fetching file:///app/tutorial/dependencies/ivy2/jars/org.antlr_ST4-4.0.8.jar with timestamp 1646255821316\n",
      "22/03/02 21:17:01 INFO Utils: /app/tutorial/dependencies/ivy2/jars/org.antlr_ST4-4.0.8.jar has been previously copied to /tmp/app/spark/spark-0a61a0b7-4b05-4402-9a81-4a9bc83aede7/userFiles-848bb9be-2112-45f1-b757-00d4f1566811/org.antlr_ST4-4.0.8.jar\n",
      "22/03/02 21:17:01 INFO Executor: Fetching file:///app/tutorial/dependencies/ivy2/jars/org.antlr_antlr4-4.7.jar with timestamp 1646255821316\n",
      "22/03/02 21:17:01 INFO Utils: /app/tutorial/dependencies/ivy2/jars/org.antlr_antlr4-4.7.jar has been previously copied to /tmp/app/spark/spark-0a61a0b7-4b05-4402-9a81-4a9bc83aede7/userFiles-848bb9be-2112-45f1-b757-00d4f1566811/org.antlr_antlr4-4.7.jar\n",
      "22/03/02 21:17:01 INFO Executor: Fetching file:///app/tutorial/dependencies/ivy2/jars/io.delta_delta-core_2.12-1.0.0.jar with timestamp 1646255821316\n",
      "22/03/02 21:17:01 INFO Utils: /app/tutorial/dependencies/ivy2/jars/io.delta_delta-core_2.12-1.0.0.jar has been previously copied to /tmp/app/spark/spark-0a61a0b7-4b05-4402-9a81-4a9bc83aede7/userFiles-848bb9be-2112-45f1-b757-00d4f1566811/io.delta_delta-core_2.12-1.0.0.jar\n",
      "22/03/02 21:17:01 INFO Executor: Fetching file:///app/tutorial/dependencies/ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar with timestamp 1646255821316\n",
      "22/03/02 21:17:01 INFO Utils: /app/tutorial/dependencies/ivy2/jars/org.antlr_antlr-runtime-3.5.2.jar has been previously copied to /tmp/app/spark/spark-0a61a0b7-4b05-4402-9a81-4a9bc83aede7/userFiles-848bb9be-2112-45f1-b757-00d4f1566811/org.antlr_antlr-runtime-3.5.2.jar\n",
      "22/03/02 21:17:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43833.\n",
      "22/03/02 21:17:02 INFO NettyBlockTransferService: Server created on fb8e47287a7e:43833\n",
      "22/03/02 21:17:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/03/02 21:17:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fb8e47287a7e, 43833, None)\n",
      "22/03/02 21:17:02 INFO BlockManagerMasterEndpoint: Registering block manager fb8e47287a7e:43833 with 4.6 GiB RAM, BlockManagerId(driver, fb8e47287a7e, 43833, None)\n",
      "22/03/02 21:17:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fb8e47287a7e, 43833, None)\n",
      "22/03/02 21:17:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fb8e47287a7e, 43833, None)\n",
      "22/03/02 21:17:02 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/app/tutorial/spark-warehouse/').\n",
      "22/03/02 21:17:02 INFO SharedState: Warehouse path is 'file:/app/tutorial/spark-warehouse/'.\n"
     ]
    }
   ],
   "source": [
    "from config import pipeline_config\n",
    "from etl_pipeline.pipeline import ETLPipeline\n",
    "import config\n",
    "from omegaconf import OmegaConf\n",
    "from pyspark import Row\n",
    "from config import columns_namespace\n",
    "from etl_pipeline.data_processor_engine.spark_engine import SparkProcessingEngine\n",
    "spark_engine = SparkProcessingEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda49033-35c3-4893-8de6-a27aa5396204",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fd2aea1-601c-4110-b69a-f0632dd61ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSPipeline(ETLPipeline):\n",
    "    def convert_raw_to_standardized(self, df):\n",
    "        return df\n",
    "\n",
    "    def transform_standardized_to_cleansed(self, data):\n",
    "        data = self.engine.set_ref_key(data)\n",
    "        data = ms_customized_merge_df360_and_WM_Account_In_Scope(self.engine)\n",
    "        data = self.engine.set_trigger_reasons(data)\n",
    "        data = self.engine.merge_with_party_and_address_relationships(data)\n",
    "        data = self.engine.set_beneficiary_hits(data)\n",
    "        data = self.engine.set_clean_names(data)\n",
    "        data = self.engine.set_concat_residue(data)\n",
    "        data = self.engine.set_concat_address_no_change(data)\n",
    "        data = self.engine.set_discovery_tokens(data)\n",
    "        return data\n",
    "\n",
    "    def transform_cleansed_to_application(self, cleansed_alert_df):\n",
    "        agent_input_agg_df = self.engine.prepare_agent_inputs(cleansed_alert_df)\n",
    "        return agent_input_agg_df\n",
    "\n",
    "\n",
    "def ms_customized_merge_df360_and_WM_Account_In_Scope(engine):\n",
    "    path = pipeline_config.WM_ACCOUNTS_IN_SCOPE_INPUT_PATH\n",
    "    WM_Account_In_Scope = engine.spark_instance.read_csv(path)\n",
    "\n",
    "    for col in WM_Account_In_Scope.columns:\n",
    "        WM_Account_In_Scope = WM_Account_In_Scope.withColumnRenamed(col, col.replace(\".\", \"_\"))\n",
    "\n",
    "    WM_Account_In_Scope.registerTempTable(\"WM_Account_In_Scope\")\n",
    "\n",
    "    df360_v2 = engine.spark_instance.spark_instance.sql(\n",
    "        \"select * from df360 left join WM_Account_In_Scope WM_Account \"\n",
    "        \"on df360.SRC_SYS_ACCT_KEY = WM_Account.AD_SRC_SYS_ACCT_KEY\"\n",
    "    )\n",
    "    return df360_v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6399f4b-e76b-4b61-9135-1068b7a83452",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_part1 = [\n",
    "    Row(\n",
    "        alert_id=1,\n",
    "        SRC_REF_KEY='2019-05-09-12.07.37.400001',\n",
    "        ACCT_NUM=1,\n",
    "        PARTY_ID=51000001,\n",
    "        ADDRESS_ID=\"A05002296231\",\n",
    "        TRIGGERED_BY=[\"John\"],\n",
    "        # AD_BNFL_NM=pattern.WL_NAME,\n",
    "        CONCAT_ADDRESS=\"Jane Doe 111 A 11TH ST AAA 1A New Jersey NJ\",\n",
    "         WL_MATCHED_TOKENS=\"[\\\"John B Doe\\\"]\",\n",
    "        ADDRESS1_COUNTRY=\"\",\n",
    "        ADDRESS1_LINE1=\"test\",\n",
    "        ADDRESS1_LINE2=\"\",\n",
    "        ADDRESS1_LINE3=\"\",\n",
    "        ADDRESS1_LINE4=\"\",\n",
    "        ADDRESS1_LINE5=\"\",\n",
    "        ACCT_REGS_NM1=\"\",\n",
    "\n",
    "    ),\n",
    "    Row(\n",
    "        alert_id=2,\n",
    "        SRC_REF_KEY='2020-01-23-08.29.18.794491',\n",
    "        ACCT_NUM=2,\n",
    "        PARTY_ID=51000002,\n",
    "        ADDRESS_ID=\"A00150805711\",\n",
    "        TRIGGERED_BY=[columns_namespace.WL_ALIASES],\n",
    "        # AD_BNFL_NM=\"AD_BNFL_NM\",\n",
    "        CONCAT_ADDRESS=\"XXX C/F YYY A ZET (DECD) Agent ZET (BENE) ABC DEF GHI 123 XYZ RD ZET XXX 11111\",\n",
    "        WL_MATCHED_TOKENS=\"[\\\"John B Doe\\\"]\",\n",
    "        ADDRESS1_COUNTRY=\"\",\n",
    "        ADDRESS1_LINE1=\"\",\n",
    "        ADDRESS1_LINE2=\"\",\n",
    "        ADDRESS1_LINE3=\"\",\n",
    "        ADDRESS1_LINE4=\"\",\n",
    "        ADDRESS1_LINE5=\"\",\n",
    "        ACCT_REGS_NM1=\"\",\n",
    "    )]\n",
    "\n",
    "data_part2 = [\n",
    "    Row(\n",
    "        alert_id=1,\n",
    "        STATUS_DESC=\"AASD\",\n",
    "        WL_NATIONALITY=\"[\\\"American\\\"]\",\n",
    "        ALERT_INTERNAL_ID=\"2\",\n",
    "        ENTITY_ID=1,\n",
    "        ENTITY_VERSION=100,\n",
    "        wl_name=\"test\",\n",
    "\n",
    "    ),\n",
    "    Row(\n",
    "        alert_id=2,\n",
    "        STATUS_DESC=\"AASD\",\n",
    "        WL_NATIONALITY=\"[\\\"American\\\"]\",\n",
    "        ALERT_INTERNAL_ID=\"3\",\n",
    "        ENTITY_ID=2,\n",
    "        ENTITY_VERSION=101,\n",
    "        wl_name=\"test\",\n",
    "\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f320226-8db0-4dc1-9c02-8c0429cd4e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_part_1 = spark_engine.spark_instance.spark_instance.createDataFrame(data_part1)\n",
    "data_part_2 = spark_engine.spark_instance.spark_instance.createDataFrame(data_part2)\n",
    "data = data_part_2.join(data_part_1, columns_namespace.ALERT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a0940f8-88a2-4eb8-a15f-a43abc91d531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alert_id</th>\n",
       "      <th>STATUS_DESC</th>\n",
       "      <th>WL_NATIONALITY</th>\n",
       "      <th>ALERT_INTERNAL_ID</th>\n",
       "      <th>ENTITY_ID</th>\n",
       "      <th>ENTITY_VERSION</th>\n",
       "      <th>wl_name</th>\n",
       "      <th>SRC_REF_KEY</th>\n",
       "      <th>ACCT_NUM</th>\n",
       "      <th>PARTY_ID</th>\n",
       "      <th>...</th>\n",
       "      <th>TRIGGERED_BY</th>\n",
       "      <th>CONCAT_ADDRESS</th>\n",
       "      <th>WL_MATCHED_TOKENS</th>\n",
       "      <th>ADDRESS1_COUNTRY</th>\n",
       "      <th>ADDRESS1_LINE1</th>\n",
       "      <th>ADDRESS1_LINE2</th>\n",
       "      <th>ADDRESS1_LINE3</th>\n",
       "      <th>ADDRESS1_LINE4</th>\n",
       "      <th>ADDRESS1_LINE5</th>\n",
       "      <th>ACCT_REGS_NM1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AASD</td>\n",
       "      <td>[\"American\"]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>test</td>\n",
       "      <td>2019-05-09-12.07.37.400001</td>\n",
       "      <td>1</td>\n",
       "      <td>51000001</td>\n",
       "      <td>...</td>\n",
       "      <td>[John]</td>\n",
       "      <td>Jane Doe 111 A 11TH ST AAA 1A New Jersey NJ</td>\n",
       "      <td>[\"John B Doe\"]</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>AASD</td>\n",
       "      <td>[\"American\"]</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "      <td>test</td>\n",
       "      <td>2020-01-23-08.29.18.794491</td>\n",
       "      <td>2</td>\n",
       "      <td>51000002</td>\n",
       "      <td>...</td>\n",
       "      <td>[WL_ALIASES]</td>\n",
       "      <td>XXX C/F YYY A ZET (DECD) Agent ZET (BENE) ABC ...</td>\n",
       "      <td>[\"John B Doe\"]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   alert_id STATUS_DESC WL_NATIONALITY ALERT_INTERNAL_ID  ENTITY_ID  \\\n",
       "0         1        AASD   [\"American\"]                 2          1   \n",
       "1         2        AASD   [\"American\"]                 3          2   \n",
       "\n",
       "   ENTITY_VERSION wl_name                 SRC_REF_KEY  ACCT_NUM  PARTY_ID  \\\n",
       "0             100    test  2019-05-09-12.07.37.400001         1  51000001   \n",
       "1             101    test  2020-01-23-08.29.18.794491         2  51000002   \n",
       "\n",
       "   ...  TRIGGERED_BY                                     CONCAT_ADDRESS  \\\n",
       "0  ...        [John]        Jane Doe 111 A 11TH ST AAA 1A New Jersey NJ   \n",
       "1  ...  [WL_ALIASES]  XXX C/F YYY A ZET (DECD) Agent ZET (BENE) ABC ...   \n",
       "\n",
       "  WL_MATCHED_TOKENS ADDRESS1_COUNTRY ADDRESS1_LINE1 ADDRESS1_LINE2  \\\n",
       "0    [\"John B Doe\"]                            test                  \n",
       "1    [\"John B Doe\"]                                                  \n",
       "\n",
       "  ADDRESS1_LINE3 ADDRESS1_LINE4 ADDRESS1_LINE5 ACCT_REGS_NM1  \n",
       "0                                                             \n",
       "1                                                             \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2120e301-89d8-45a9-a5b6-052367bd3dcc",
   "metadata": {},
   "source": [
    "### Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "360ce539-9820-4ddf-a9d9-6f48cecf165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = OmegaConf.load(\"config/config_app.yaml\")\n",
    "pipeline_config = conf.PIPELINE\n",
    "columns_namespace = conf.SPARK_DATAFRAME_COLUMNS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2080aa37-a574-4e0f-a849-a90a9a15030a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output-paths': 'output-paths', 'alerts-data': 'alerts-data', 'alerts-notes': 'alerts-notes', 'alerts-xml-data': 'alerts-xml-data', 'temp': '/tmp', 'sanctions-360': 'sanctions-360', 'input-record-columns': 'input-record-columns', 'id-column-name': 'id-column-name', 'WM_ACCOUNTS_IN_SCOPE_INPUT_PATH': '../ms-name-screening-poc/test-data/Sanctions/WM/WM_Account_In_Scope.csv', 'WM_PARTIES_IN_SCOPE_INPUT_PATH': '../ms-name-screening-poc/test-data/Sanctions/WM/WM_Parties_In_Scope.csv', 'df360-input-path': 'df360-input-path', 'df360-output-path': 'df360-output-path', 'fuzziness': 100, 'PARTY_AND_ADDRESS_RELATIONSHIP_INPUT_PATH': '../tests/shared/Party_And_Address_Relationship.csv', 'decision-tree': 'decision-tree', 'alerts': 'alerts', 'alert-type': 'alert-type', 'WM_ADDRESS_COLS_TO_CONVERT_FROM_JSON': [], 'ISG_ACCOUNT_COLS_TO_CONVERT_FROM_JSON': [], 'AP_COLUMNS': ['ADDRESS1_COUNTRY', 'ADDRESS1_LINE1', 'ADDRESS1_LINE2', 'ADDRESS1_LINE3', 'ADDRESS1_LINE4', 'ADDRESS1_LINE5', 'CONCAT_ADDRESS', 'ALL_PARTY_NAMES', 'ACCT_REGS_NM1', 'concat_residue', 'concat_address_no_changes', 'all_connected_parties_names'], 'REFERENCES_COLUMNS': ['PRTY_TYP', 'PRTY_NM', 'DOB_DT', 'PRTY_CNTRY_OF_BIRTH', 'PRTY_PRIM_CTZNSH_CNTRY', 'PRTY_RSDNC_CNTRY_CD']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a158ea-544c-4734-8e88-149c3d97377d",
   "metadata": {},
   "source": [
    "# Pipeline example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5b445a5-1bb0-4930-a7e1-231366b34462",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSPipeline(ETLPipeline):\n",
    "    def convert_raw_to_standardized(self, df):\n",
    "        return df\n",
    "\n",
    "    def transform_standardized_to_cleansed(self, data):\n",
    "        data = self.engine.set_ref_key(data)\n",
    "        data.registerTempTable(\"df360\")\n",
    "        data = self.ms_customized_merge_df360_and_WM_Account_In_Scope()\n",
    "        data = self.engine.set_trigger_reasons(data)\n",
    "        data = self.engine.merge_with_party_and_address_relationships(data)\n",
    "        data = self.engine.set_beneficiary_hits(data)\n",
    "        data = self.engine.set_clean_names(data)\n",
    "        data = self.engine.set_concat_residue(data)\n",
    "        data = self.engine.set_concat_address_no_change(data)\n",
    "        data = self.engine.set_discovery_tokens(data)\n",
    "        return data\n",
    "\n",
    "    def transform_cleansed_to_application(self, cleansed_alert_df):\n",
    "        agent_input_agg_df = self.engine.prepare_agent_inputs(cleansed_alert_df)\n",
    "        return agent_input_agg_df\n",
    "\n",
    "    def ms_customized_merge_df360_and_WM_Account_In_Scope(self):\n",
    "        path = self.pipeline_config.WM_ACCOUNTS_IN_SCOPE_INPUT_PATH\n",
    "        WM_Account_In_Scope = self.engine.spark_instance.read_csv(path)\n",
    "\n",
    "        for col in WM_Account_In_Scope.columns:\n",
    "            WM_Account_In_Scope = WM_Account_In_Scope.withColumnRenamed(col, col.replace(\".\", \"_\"))\n",
    "\n",
    "        WM_Account_In_Scope.registerTempTable(\"WM_Account_In_Scope\")\n",
    "\n",
    "        df360_v2 = self.engine.spark_instance.spark_instance.sql(\n",
    "            \"select * from df360 left join WM_Account_In_Scope WM_Account \"\n",
    "            \"on df360.SRC_SYS_ACCT_KEY = WM_Account.AD_SRC_SYS_ACCT_KEY\"\n",
    "        )\n",
    "        return df360_v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89333c90-d962-4d73-a8e6-fa983cb47355",
   "metadata": {},
   "outputs": [],
   "source": [
    "uut = MSPipeline(spark_engine, config=pipeline_config)\n",
    "result = uut.transform_standardized_to_cleansed(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f942bf9-2cc1-4dfd-a795-9d64a4861f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = uut.transform_cleansed_to_application(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "586c08ad-814a-4ea3-8400-73fb123f3dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/ds/anaconda/envs/pipeline/lib/python3.7/site-packages/pyspark/sql/pandas/conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    }
   ],
   "source": [
    "pd_result = result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f2a113-436c-4ea5-84e4-fe46aa43e513",
   "metadata": {},
   "source": [
    "# Prepare agent input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7e863d1-e795-4085-81bf-fce368708662",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3968094792.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_573851/3968094792.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    package silenteight.datasource.api.date.v1;\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "syntax = \"proto3\";\n",
    "\n",
    "package silenteight.datasource.api.date.v1;\n",
    "\n",
    "import \"google/api/annotations.proto\";\n",
    "\n",
    "option java_package = \"com.silenteight.datasource.api.date.v1\";\n",
    "option java_outer_classname = \"DateProto\";\n",
    "option java_multiple_files = true;\n",
    "\n",
    "service DateInputService {\n",
    "  rpc BatchGetMatchDateInputs (BatchGetMatchDateInputsRequest) returns (stream BatchGetMatchDateInputsResponse) {\n",
    "    option (google.api.http) = {\n",
    "      post: \"/agent-inputs/date/versions/v1/matches:batchGet\"\n",
    "      body: \"*\"\n",
    "    };\n",
    "  }\n",
    "}\n",
    "\n",
    "message BatchGetMatchDateInputsRequest {\n",
    "  // The list of matches data is requested for, in form of `alerts/<Alert ID>/matches/<Match ID>`.\n",
    "  repeated string matches = 1;\n",
    "  // The list of features the input data for each match is requested, in form of `features/<Feature ID>`.\n",
    "  repeated string features = 2;\n",
    "}\n",
    "\n",
    "message BatchGetMatchDateInputsResponse {\n",
    "  repeated DateInput date_inputs = 1;\n",
    "}\n",
    "\n",
    "message DateInput {\n",
    "  string match = 1;\n",
    "  repeated DateFeatureInput date_feature_inputs = 2;\n",
    "}\n",
    "\n",
    "message DateFeatureInput {\n",
    "  string feature = 1;\n",
    "  repeated string alerted_party_dates = 2;\n",
    "  repeated string watchlist_dates = 3;\n",
    "\n",
    "  EntityType alerted_party_type = 4;\n",
    "  SeverityMode mode = 5;\n",
    "\n",
    "  enum EntityType {\n",
    "    ENTITY_TYPE_UNSPECIFIED = 0;\n",
    "    INDIVIDUAL = 1;\n",
    "    ORGANIZATION = 2;\n",
    "  }\n",
    "\n",
    "  enum SeverityMode {\n",
    "    NORMAL = 0;\n",
    "    STRICT = 1;\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2375de-7309-4f96-8d02-8d3e0b66eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "message DateFeatureInput {\n",
    "  string feature = 1;\n",
    "  repeated string alerted_party_dates = 2;\n",
    "  repeated string watchlist_dates = 3;\n",
    "\n",
    "  EntityType alerted_party_type = 4;\n",
    "  SeverityMode mode = 5;\n",
    "\n",
    "  enum EntityType {\n",
    "    ENTITY_TYPE_UNSPECIFIED = 0;\n",
    "    INDIVIDUAL = 1;\n",
    "    ORGANIZATION = 2;\n",
    "  }\n",
    "\n",
    "  enum SeverityMode {\n",
    "    NORMAL = 0;\n",
    "    STRICT = 1;\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59ebfa96-41c0-40f2-98e5-bdac2133a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_type = [column for column in pd_result.columns if \"alerted_party_type\" in column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ecc7eb65-76e3-4fe6-9afc-a37df697356f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[column for column in pd_result.columns if \"severity\" in column.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64d6ae7f-e99a-4b8b-93b2-7057c7b9df97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALL_PARTY_TYPES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Individual, Individual]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Individual]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ALL_PARTY_TYPES\n",
       "0  [Individual, Individual]\n",
       "1              [Individual]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_result[[column for column in pd_result.columns if \"ALL_PARTY_TYPES\" in column]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb0348d4-ed14-4c94-a590-ff37433ce812",
   "metadata": {},
   "outputs": [],
   "source": [
    "dob_columns = [column for column in pd_result.columns if \"dob\" in column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4b59aaa-41e6-49c1-bc37-ffb9e0771a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = pd_result[dob_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40a09b4a-40cf-47a5-8b0f-9efb12431e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[list(['01/02/1972', '01/02/1938']), None,\n",
       "        list(['01/02/1972', '01/02/1938']), list([])],\n",
       "       [list(['01/02/1960']), None, list(['01/02/1960']), list([])]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a97921e4-94fc-448d-9387-5ccac754adac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from silenteight.datasource.api.date.v1.date_pb2 import DateFeatureInput\n",
    "from silenteight.datasource.agentinput.api.v1.agent_input_pb2 import AgentInput, FeatureInput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e2f2dee-6d89-4ade-89ef-eef48af2231c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m  \u001b[0mDateFeatureInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      A ProtocolMessage\n",
       "\u001b[0;31mFile:\u001b[0m           /env/ds/anaconda/envs/pipeline/lib/python3.7/site-packages/silenteight/datasource/api/date/v1/date_pb2.py\n",
       "\u001b[0;31mType:\u001b[0m           GeneratedProtocolMessageType\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "? DateFeatureInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fa1504dc-4f1f-4a98-948e-8a668ed60805",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = DateFeatureInput(feature=\"dob\", alerted_party_dates=item[0][0], watchlist_dates=item[0][1], alerted_party_type=1, mode=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "283ebea5-eaef-4b5f-a035-0615f239b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.protobuf.any_pb2 import Any\n",
    "target =  Any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fc09ec53-9728-4016-ba83-fcba42f3573c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature: \"dob\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FeatureInput(feature=message.feature, agent_feature_input=target.Pack((message)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3866e7b-406d-4912-9832-51098bac665a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
